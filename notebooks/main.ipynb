{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Design Choices Using Fashion-MNIST\n",
    "\n",
    "**Portfolio Piece 1** — Extending Lab 2 (Neural Network Exploration)\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation\n",
    "\n",
    "In Lab 2, we built simple neural networks on XOR and MNIST to see gradient descent in action. The results were encouraging, but MNIST digit classification is essentially a *solved* problem — even a basic MLP hits 97%+, which leaves little room to see how design choices actually matter.\n",
    "\n",
    "**Fashion-MNIST** changes this. Introduced by Zalando Research as a drop-in MNIST replacement, it uses the same 28×28 grayscale format but presents a much harder task: distinguishing t-shirts from pullovers, sneakers from ankle boots, and coats from shirts. These classes have real visual overlap, which means architecture, optimization, and regularization decisions have *measurable impact* on performance.\n",
    "\n",
    "This notebook systematically investigates three questions:\n",
    "1. **Architecture**: How do depth, width, activation functions, and skip connections affect what a network can learn?\n",
    "2. **Optimization**: How do different optimizers, learning rate schedules, and regularization strategies change the training dynamics?\n",
    "3. **Interpretability**: What is the network actually learning, and where does it fail?\n",
    "\n",
    "Each experiment is designed to isolate a single variable so we can draw meaningful conclusions about *why* certain choices work better than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import sys, os\n",
    "\n",
    "# Add src/ to path so we can import our utilities\n",
    "sys.path.append(os.path.join(os.pardir, 'src'))\n",
    "from utils import (\n",
    "    train_model, plot_training_curves, plot_confusion_matrix,\n",
    "    extract_features, plot_tsne, compare_experiments, count_parameters\n",
    ")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST class names\n",
    "CLASS_NAMES = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "# Standard normalization for Fashion-MNIST (mean=0.2860, std=0.3530)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.FashionMNIST(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Test samples:     {len(test_dataset):,}\")\n",
    "print(f\"Image shape:      {train_dataset[0][0].shape}\")\n",
    "print(f\"Classes:          {len(CLASS_NAMES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualizing the Dataset\n",
    "\n",
    "Let's look at sample images from each class to build intuition about the task difficulty. Pay attention to how visually similar some classes are — this will matter when we analyze model errors later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 3 examples per class\n",
    "fig, axes = plt.subplots(3, 10, figsize=(16, 5))\n",
    "fig.suptitle('Fashion-MNIST: 3 Random Samples Per Class', fontsize=14, y=1.02)\n",
    "\n",
    "for class_idx in range(10):\n",
    "    # Find indices for this class\n",
    "    class_indices = [i for i, (_, label) in enumerate(train_dataset) if label == class_idx]\n",
    "    samples = np.random.choice(class_indices, 3, replace=False)\n",
    "    \n",
    "    for row, idx in enumerate(samples):\n",
    "        img, _ = train_dataset[idx]\n",
    "        axes[row, class_idx].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[row, class_idx].axis('off')\n",
    "        if row == 0:\n",
    "            axes[row, class_idx].set_title(CLASS_NAMES[class_idx], fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/dataset_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "label_counts = Counter(train_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "bars = ax.bar(CLASS_NAMES, [label_counts[i] for i in range(10)], color=sns.color_palette('viridis', 10))\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Training Set Class Distribution')\n",
    "ax.set_xticklabels(CLASS_NAMES, rotation=30, ha='right')\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "            f'{int(bar.get_height()):,}', ha='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe dataset is perfectly balanced — 6,000 samples per class.\")\n",
    "print(\"This means accuracy is a fair metric (no class imbalance to worry about).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pixel Intensity Analysis\n",
    "\n",
    "Understanding the raw data helps us appreciate what the network has to work with. Let's look at the average image per class — this reveals which classes have the most visual overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean image per class (using raw pixels, not normalized)\n",
    "raw_transform = transforms.ToTensor()\n",
    "raw_dataset = datasets.FashionMNIST(root='../data', train=True, download=False, transform=raw_transform)\n",
    "\n",
    "mean_images = {}\n",
    "for class_idx in range(10):\n",
    "    class_imgs = torch.stack([raw_dataset[i][0] for i in range(len(raw_dataset)) if raw_dataset[i][1] == class_idx])\n",
    "    mean_images[class_idx] = class_imgs.mean(dim=0).squeeze()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "fig.suptitle('Mean Image Per Class — Notice the Overlap Between Similar Categories', fontsize=13)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(mean_images[idx], cmap='hot')\n",
    "    ax.set_title(CLASS_NAMES[idx], fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/mean_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation: T-shirt, Pullover, Coat, and Shirt have very similar mean images.\")\n",
    "print(\"This suggests the network must learn subtle features to distinguish them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Baseline Model\n",
    "\n",
    "We start with a simple 2-layer MLP — similar to what we built in Lab 2 for MNIST. This establishes a performance floor and gives us a reference point for all subsequent experiments.\n",
    "\n",
    "**Design choice**: We flatten the 28×28 image into a 784-dimensional vector. This discards spatial information entirely, but it's the simplest possible approach and makes a good baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineMLP(nn.Module):\n",
    "    \"\"\"Simple 2-layer MLP. Mirrors what we built in Lab 2.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(self.flatten(x))\n",
    "\n",
    "# Train baseline\n",
    "EPOCHS = 30\n",
    "baseline_model = BaselineMLP().to(device)\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "total_params, _ = count_parameters(baseline_model)\n",
    "print(f\"Baseline MLP: {total_params:,} parameters\")\n",
    "print(f\"Training for {EPOCHS} epochs...\\n\")\n",
    "\n",
    "baseline_history = train_model(\n",
    "    baseline_model, train_loader, test_loader, optimizer, criterion,\n",
    "    device, epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(baseline_history, title='Baseline MLP (784→128→64→10)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline gives us a starting point. Notice how quickly the gap between train and validation accuracy appears — the model starts overfitting early, which motivates our regularization experiments later.\n",
    "\n",
    "---\n",
    "## 3. Architecture Experiments\n",
    "\n",
    "Now we systematically vary one architectural dimension at a time. This is critical for understanding *what* each design choice contributes — changing multiple things at once makes it impossible to attribute improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Experiment: Network Depth\n",
    "\n",
    "**Question**: Does adding more layers help? When does it start to hurt?\n",
    "\n",
    "Theory: Deeper networks can learn more abstract hierarchical features, but they're also harder to train due to vanishing/exploding gradients. With plain MLPs (no skip connections), we should see diminishing returns or degradation beyond a certain depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP with configurable depth, width, and activation.\n",
    "    All hidden layers use the same width for controlled experiments.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10,\n",
    "                 num_hidden=2, activation='relu', dropout=0.0, use_batchnorm=False):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Map activation names to functions\n",
    "        act_fn = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'sigmoid': nn.Sigmoid()\n",
    "        }[activation]\n",
    "        \n",
    "        layers = []\n",
    "        # First hidden layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "        layers.append(act_fn)\n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Additional hidden layers\n",
    "        for _ in range(num_hidden - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(act_fn)\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Output\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(self.flatten(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth experiment: 1, 2, 3, 5, 8 hidden layers (all width=128)\n",
    "depth_results = {}\n",
    "depth_configs = [1, 2, 3, 5, 8]\n",
    "\n",
    "for n_layers in depth_configs:\n",
    "    name = f\"{n_layers} hidden layer{'s' if n_layers > 1 else ''}\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    \n",
    "    model = FlexibleMLP(num_hidden=n_layers, hidden_dim=128).to(device)\n",
    "    total, _ = count_parameters(model)\n",
    "    print(f\"  Parameters: {total:,}\")\n",
    "    \n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    history = train_model(model, train_loader, test_loader, opt, criterion, device, epochs=EPOCHS)\n",
    "    depth_results[name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_experiments(depth_results)\n",
    "plt.savefig('../outputs/depth_comparison.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay all depth loss curves for direct comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = sns.color_palette('viridis', len(depth_results))\n",
    "\n",
    "for (name, hist), color in zip(depth_results.items(), colors):\n",
    "    epochs = range(1, len(hist['val_loss']) + 1)\n",
    "    ax1.plot(epochs, hist['val_loss'], label=name, color=color, linewidth=2)\n",
    "    ax2.plot(epochs, hist['val_acc'], label=name, color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Validation Loss')\n",
    "ax1.set_title('Depth Experiment — Validation Loss'); ax1.legend(fontsize=8)\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Validation Accuracy (%)')\n",
    "ax2.set_title('Depth Experiment — Validation Accuracy'); ax2.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/depth_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis — Depth:**\n",
    "\n",
    "Examine the results above and note:\n",
    "- How does validation accuracy change as we add layers?\n",
    "- At what depth do we start seeing diminishing returns or degradation?\n",
    "- Look at the loss curves: do deeper models overfit more or less?\n",
    "- The 8-layer model likely trains slower and may perform worse — this is the **degradation problem** that motivated residual networks (He et al., 2015).\n",
    "\n",
    "*Your observations:* [Add your analysis after running]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Experiment: Network Width\n",
    "\n",
    "**Question**: Is it better to go wider or deeper? How much capacity does this task actually need?\n",
    "\n",
    "We fix depth at 2 hidden layers and vary width. This tests the model's *capacity* — its ability to represent complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_results = {}\n",
    "width_configs = [32, 64, 128, 256, 512]\n",
    "\n",
    "for width in width_configs:\n",
    "    name = f\"Width {width}\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    \n",
    "    model = FlexibleMLP(num_hidden=2, hidden_dim=width).to(device)\n",
    "    total, _ = count_parameters(model)\n",
    "    print(f\"  Parameters: {total:,}\")\n",
    "    \n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    history = train_model(model, train_loader, test_loader, opt, criterion, device, epochs=EPOCHS)\n",
    "    width_results[name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_experiments(width_results)\n",
    "plt.savefig('../outputs/width_comparison.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis — Width:**\n",
    "\n",
    "- Width 32 is likely underfitting (not enough capacity). Width 512 probably overfits faster.\n",
    "- There's a sweet spot: enough capacity to model the task, but not so much that the model memorizes the training set.\n",
    "- Compare parameter counts: Width 512 has ~16x more parameters than Width 32. Is the accuracy gain proportional?\n",
    "- This illustrates the **bias-variance tradeoff** in a concrete, measurable way.\n",
    "\n",
    "*Your observations:* [Add your analysis after running]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Experiment: Activation Functions\n",
    "\n",
    "**Question**: How much does the choice of activation function matter?\n",
    "\n",
    "We test five activations with identical architectures (2 layers, width 128). Each activation has different properties: ReLU can cause dead neurons, sigmoid saturates, GELU is the modern default in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_results = {}\n",
    "activations = ['relu', 'tanh', 'leaky_relu', 'gelu', 'sigmoid']\n",
    "\n",
    "for act in activations:\n",
    "    name = act.upper() if act != 'leaky_relu' else 'LeakyReLU'\n",
    "    if act == 'gelu': name = 'GELU'\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    \n",
    "    model = FlexibleMLP(num_hidden=2, hidden_dim=128, activation=act).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    history = train_model(model, train_loader, test_loader, opt, criterion, device, epochs=EPOCHS)\n",
    "    activation_results[name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_experiments(activation_results)\n",
    "plt.savefig('../outputs/activation_comparison.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis — Activation Functions:**\n",
    "\n",
    "- Sigmoid likely underperforms due to the **vanishing gradient** problem: its gradients are tiny for large/small inputs.\n",
    "- ReLU, LeakyReLU, and GELU should perform similarly. The differences are more pronounced in deeper networks.\n",
    "- Tanh centers outputs around zero (which can help), but also suffers from saturation.\n",
    "- GELU is a smooth approximation of ReLU that allows small negative values — it's become the default in modern architectures (GPT, BERT) for good reason.\n",
    "\n",
    "*Your observations:* [Add your analysis after running]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Experiment: Skip Connections\n",
    "\n",
    "**Question**: Can residual connections help plain MLPs?\n",
    "\n",
    "Skip (residual) connections let gradients flow directly through the network, addressing the degradation problem we likely observed in the depth experiment. Let's test whether adding residual connections to a deep MLP recovers the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A single residual block: Linear → BN → ReLU → Linear → BN + skip.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.block(x) + x)  # The skip connection\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    \"\"\"MLP with residual connections. Comparable depth to the deep plain MLPs.\"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10, num_blocks=3):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_dim) for _ in range(num_blocks)])\n",
    "        self.head = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_results = {}\n",
    "\n",
    "# Deep plain MLP (8 layers) for comparison\n",
    "print(\"Training: Deep Plain MLP (8 layers)\")\n",
    "plain_deep = FlexibleMLP(num_hidden=8, hidden_dim=128).to(device)\n",
    "opt = optim.Adam(plain_deep.parameters(), lr=1e-3)\n",
    "skip_results['Plain 8-layer'] = train_model(\n",
    "    plain_deep, train_loader, test_loader, opt, criterion, device, epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# Residual MLP (3 blocks = 6 layers effective depth + input proj)\n",
    "print(\"\\nTraining: Residual MLP (3 blocks)\")\n",
    "res_model = ResidualMLP(num_blocks=3).to(device)\n",
    "total, _ = count_parameters(res_model)\n",
    "print(f\"  Parameters: {total:,}\")\n",
    "opt = optim.Adam(res_model.parameters(), lr=1e-3)\n",
    "skip_results['Residual 3-block'] = train_model(\n",
    "    res_model, train_loader, test_loader, opt, criterion, device, epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# Residual MLP (5 blocks = deeper)\n",
    "print(\"\\nTraining: Residual MLP (5 blocks)\")\n",
    "res_model_5 = ResidualMLP(num_blocks=5).to(device)\n",
    "opt = optim.Adam(res_model_5.parameters(), lr=1e-3)\n",
    "skip_results['Residual 5-block'] = train_model(\n",
    "    res_model_5, train_loader, test_loader, opt, criterion, device, epochs=EPOCHS\n",
    ")\n",
    "\n",
    "compare_experiments(skip_results)\n",
    "plt.savefig('../outputs/skip_connection_comparison.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis — Skip Connections:**\n",
    "\n",
    "- Compare the residual models to the plain 8-layer network. The residual versions should train more stably and achieve higher accuracy.\n",
    "- The key insight: skip connections don't add capacity — they add **trainability**. The network can always learn the identity mapping (set residual weights to zero), so depth never *hurts*.\n",
    "- This is the same principle behind ResNets, which enabled training of 100+ layer networks for image classification.\n",
    "\n",
    "*Your observations:* [Add your analysis after running]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Optimization Deep-Dive\n",
    "\n",
    "Architecture defines *what* the model can learn; optimization determines *how well* it actually learns it. We'll now fix a good architecture and experiment with the training recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Experiment: Optimizer Comparison\n",
    "\n",
    "**Question**: How do different optimizers affect convergence speed and final performance?\n",
    "\n",
    "We compare four popular optimizers, each representing a different approach to navigating the loss landscape:\n",
    "- **SGD**: Vanilla stochastic gradient descent — the simplest possible approach\n",
    "- **SGD + Momentum**: Adds \"inertia\" to avoid oscillation in ravines\n",
    "- **Adam**: Adaptive learning rates per-parameter + momentum\n",
    "- **AdamW**: Adam with proper weight decay (decoupled from the gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results = {}\n",
    "\n",
    "optimizer_configs = {\n",
    "    'SGD (lr=0.01)': lambda params: optim.SGD(params, lr=0.01),\n",
    "    'SGD + Momentum': lambda params: optim.SGD(params, lr=0.01, momentum=0.9),\n",
    "    'Adam (lr=1e-3)': lambda params: optim.Adam(params, lr=1e-3),\n",
    "    'AdamW (lr=1e-3)': lambda params: optim.AdamW(params, lr=1e-3, weight_decay=1e-2)\n",
    "}\n",
    "\n",
    "for name, make_opt in optimizer_configs.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    \n",
    "    model = FlexibleMLP(num_hidden=2, hidden_dim=256).to(device)\n",
    "    opt = make_opt(model.parameters())\n",
    "    history = train_model(model, train_loader, test_loader, opt, criterion, device, epochs=EPOCHS)\n",
    "    opt_results[name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = sns.color_palette('Set2', len(opt_results))\n",
    "\n",
    "for (name, hist), color in zip(opt_results.items(), colors):\n",
    "    epochs = range(1, len(hist['train_loss']) + 1)\n",
    "    ax1.plot(epochs, hist['train_loss'], label=name, color=color, linewidth=2)\n",
    "    ax2.plot(epochs, hist['val_acc'], label=name, color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Training Loss')\n",
    "ax1.set_title('Optimizer Comparison — Training Loss'); ax1.legend(fontsize=9)\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Validation Accuracy (%)')\n",
    "ax2.set_title('Optimizer Comparison — Validation Accuracy'); ax2.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/optimizer_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis — Optimizers:**\n",
    "\n",
    "- SGD converges slowly but can find flatter minima (better generalization). \n",
    "- Adam converges fast due to adaptive learning rates but may overfit more.\n",
    "- AdamW adds weight decay to Adam — this regularizes the model and often matches or beats Adam.\n",
    "- Momentum helps SGD enormously by smoothing the gradient direction.\n",
    "\n",
    "*Your observations:* [Add your analysis after running]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Experiment: Learning Rate Schedules\n",
    "\n",
    "**Question**: Does decaying the learning rate during training help?\n",
    "\n",
    "The intuition: start with a large learning rate to make fast progress, then reduce it to fine-tune and settle into a good minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_results = {}\n",
    "\n",
    "# Fixed LR\n",
    "print(\"Training: Constant LR\")\n",
    "model = FlexibleMLP(num_hidden=2, hidden_dim=256).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "schedule_results['Constant LR'] = train_model(\n",
    "    model, train_loader, test_loader, opt, criterion, device, epochs=40\n",
    ")\n",
    "\n",
    "# StepLR (decay by 0.5 every 10 epochs)\n",
    "print(\"\\nTraining: StepLR (×0.5 every 10 epochs)\")\n",
    "model = FlexibleMLP(num_hidden=2, hidden_dim=256).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "sched = optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "schedule_results['StepLR'] = train_model(\n",
    "    model, train_loader, test_loader, opt, criterion, device, epochs=40, scheduler=sched\n",
    ")\n",
    "\n",
    "# CosineAnnealingLR\n",
    "print(\"\\nTraining: Cosine Annealing\")\n",
    "model = FlexibleMLP(num_hidden=2, hidden_dim=256).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=40)\n",
    "schedule_results['Cosine Annealing'] = train_model(\n",
    "    model, train_loader, test_loader, opt, criterion, device, epochs=40, scheduler=sched\n",
    ")\n",
    "\n",
    "compare_experiments(schedule_results)\n",
    "plt.savefig('../outputs/schedule_comparison.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Experiment: Regularization\n",
    "\n",
    "**Question**: The baseline overfits. Can regularization close the train-val gap?\n",
    "\n",
    "We test three complementary strategies:\n",
    "- **Dropout**: Randomly zeroes neurons during training, forcing redundancy\n",
    "- **Weight Decay (L2)**: Penalizes large weights, encouraging simpler solutions\n",
    "- **Batch Normalization**: Normalizes intermediate activations, stabilizing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_results = {}\n",
    "\n",
    "# No regularization\n",
    "print(\"Training: No regularization\")\n",
    "model = FlexibleMLP(num_hidden=3, hidden_dim=256).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "reg_results['No regularization'] = train_model(\n",
    "    model, train_loader, test_loader, opt, criterion, device, epochs=40\n",
    ")\n",
    "\n",
    "# Dropout only\n",
    "print(\"\\nTraining: Dropout 0.3\")\n",
    "model = FlexibleMLP(num_hidden=3, hidden_dim=256, dropout=0.3).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "reg_results['Dropout 0.3'] = train_model(\n",
    "    model, train_loader, test_loader, opt, criterion, device, epochs=40\n",
    ")\n",
    "\n",
    "# Weight decay only\n",
    "print(\"\\nTraining: Weight Decay 1e-2\")\n",
    "model = FlexibleMLP(num_hidden=3, hidden_dim=256).to(device)\n",
    "opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "reg_results['Weight Decay'] = train_model(\n",
    "    model, train_loader, test_loader, opt, criterion, device, epochs=40\n",
    ")\n",
    "\n",
    "# BatchNorm only\n",
    "print(\"\\nTraining: BatchNorm\")\n",
    "model = FlexibleMLP(num_hidden=3, hidden_dim=256, use_batchnorm=True).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "reg_results['BatchNorm'] = train_model(\n",
    "    model, train_loader, test_loader, opt, criterion, device, epochs=40\n",
    ")\n",
    "\n",
    "# All combined\n",
    "print(\"\\nTraining: Dropout + Weight Decay + BatchNorm\")\n",
    "model = FlexibleMLP(num_hidden=3, hidden_dim=256, dropout=0.3, use_batchnorm=True).to(device)\n",
    "opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "reg_results['All Combined'] = train_model(\n",
    "    model, train_loader, test_loader, opt, criterion, device, epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_experiments(reg_results)\n",
    "plt.savefig('../outputs/regularization_comparison.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overfitting gap for each regularization method\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "colors = sns.color_palette('Set2', len(reg_results))\n",
    "\n",
    "for (name, hist), color in zip(reg_results.items(), colors):\n",
    "    gap = [t - v for t, v in zip(hist['train_acc'], hist['val_acc'])]\n",
    "    ax.plot(range(1, len(gap)+1), gap, label=name, color=color, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Train Acc - Val Acc (% gap)')\n",
    "ax.set_title('Generalization Gap: How Much is Each Model Overfitting?')\n",
    "ax.legend(fontsize=9)\n",
    "ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/generalization_gap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis — Regularization:**\n",
    "\n",
    "- The generalization gap plot is key: it shows how much each method reduces the train-val accuracy difference.\n",
    "- Dropout forces the network to learn redundant representations — no single neuron can be a \"crutch.\"\n",
    "- BatchNorm smooths the optimization landscape, which often implicitly regularizes.\n",
    "- Combining all methods should give the smallest gap without sacrificing too much training speed.\n",
    "\n",
    "*Your observations:* [Add your analysis after running]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Visualization & Interpretability\n",
    "\n",
    "Numbers and curves tell us *how well* the model does. Now let's understand *what* it's actually learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Best Model — Confusion Matrix\n",
    "\n",
    "Train the best configuration we've found so far, then examine where it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our best model: Residual MLP with full regularization\n",
    "print(\"Training best model: Residual MLP + Dropout + BatchNorm + AdamW + Cosine LR\")\n",
    "best_model = ResidualMLP(num_blocks=3, hidden_dim=256).to(device)\n",
    "\n",
    "# Add dropout to residual model manually\n",
    "class BestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.blocks = nn.Sequential(\n",
    "            ResidualBlock(256),\n",
    "            nn.Dropout(0.2),\n",
    "            ResidualBlock(256),\n",
    "            nn.Dropout(0.2),\n",
    "            ResidualBlock(256),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.head = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x)\n",
    "\n",
    "best_model = BestModel().to(device)\n",
    "total, _ = count_parameters(best_model)\n",
    "print(f\"Parameters: {total:,}\")\n",
    "\n",
    "opt = optim.AdamW(best_model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=50)\n",
    "best_history = train_model(\n",
    "    best_model, train_loader, test_loader, opt, criterion, device, epochs=50, scheduler=sched\n",
    ")\n",
    "\n",
    "# Restore best weights\n",
    "best_model.load_state_dict(best_history['best_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(best_history, title='Best Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plot_confusion_matrix(best_model, test_loader, device, CLASS_NAMES, title='Best Model — Confusion Matrix')\n",
    "plt.savefig('../outputs/confusion_matrix.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy breakdown\n",
    "from utils import evaluate\n",
    "_, _, preds, labels = evaluate(best_model, test_loader, criterion, device)\n",
    "\n",
    "print(\"Per-Class Accuracy:\")\n",
    "print(\"=\" * 40)\n",
    "class_acc = []\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    mask = labels == i\n",
    "    acc = (preds[mask] == labels[mask]).mean() * 100\n",
    "    class_acc.append(acc)\n",
    "    print(f\"  {name:15s}: {acc:.1f}%\")\n",
    "\n",
    "print(f\"\\n  {'Overall':15s}: {(preds == labels).mean() * 100:.1f}%\")\n",
    "print(f\"\\n  Hardest class:  {CLASS_NAMES[np.argmin(class_acc)]} ({min(class_acc):.1f}%)\")\n",
    "print(f\"  Easiest class:  {CLASS_NAMES[np.argmax(class_acc)]} ({max(class_acc):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Misclassification Analysis\n",
    "\n",
    "Let's look at specific examples the model gets wrong. This gives qualitative insight into the model's failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "best_model.eval()\n",
    "misclassified = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        img, label = test_dataset[i]\n",
    "        output = best_model(img.unsqueeze(0).to(device))\n",
    "        pred = output.argmax(1).item()\n",
    "        conf = torch.softmax(output, dim=1).max().item()\n",
    "        if pred != label:\n",
    "            misclassified.append((img, label, pred, conf))\n",
    "\n",
    "print(f\"Total misclassified: {len(misclassified)} / {len(test_dataset)} ({100*len(misclassified)/len(test_dataset):.1f}%)\")\n",
    "\n",
    "# Show 15 random misclassifications\n",
    "fig, axes = plt.subplots(3, 5, figsize=(14, 8))\n",
    "fig.suptitle('Misclassified Examples: True → Predicted (Confidence)', fontsize=13)\n",
    "\n",
    "sample = np.random.choice(len(misclassified), min(15, len(misclassified)), replace=False)\n",
    "for ax, idx in zip(axes.flat, sample):\n",
    "    img, true, pred, conf = misclassified[idx]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"{CLASS_NAMES[true]}\\n→ {CLASS_NAMES[pred]} ({conf:.0%})\", fontsize=8,\n",
    "                 color='red' if conf > 0.8 else 'orange')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/misclassified_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Learned Representations (t-SNE)\n",
    "\n",
    "We extract the activations from the penultimate layer and project them to 2D using t-SNE. If the model has learned good features, we should see clear clusters per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare representations: baseline vs best model\n",
    "print(\"Extracting features from baseline model...\")\n",
    "baseline_model.load_state_dict(baseline_history['best_model'])\n",
    "baseline_feats, baseline_labels = extract_features(baseline_model, test_loader, device)\n",
    "plot_tsne(baseline_feats, baseline_labels, CLASS_NAMES, title='t-SNE: Baseline MLP Representations')\n",
    "plt.savefig('../outputs/tsne_baseline.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features from best model...\")\n",
    "best_feats, best_labels = extract_features(best_model, test_loader, device)\n",
    "plot_tsne(best_feats, best_labels, CLASS_NAMES, title='t-SNE: Best Model Representations')\n",
    "plt.savefig('../outputs/tsne_best.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis — Representations:**\n",
    "\n",
    "Compare the two t-SNE plots:\n",
    "- The best model should show tighter, more separated clusters.\n",
    "- Notice which classes overlap — these are the same pairs that confuse the classifier in the confusion matrix.\n",
    "- Footwear (Sandal, Sneaker, Ankle boot) and upper-body clothing (T-shirt, Pullover, Coat, Shirt) likely form two super-clusters.\n",
    "\n",
    "*Your observations:* [Add your analysis after running]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Weight Distribution Analysis\n",
    "\n",
    "Visualizing the distribution of weights across layers can reveal training health issues like vanishing gradients or poorly initialized layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weight distributions for each layer\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "fig.suptitle('Weight Distributions Across Layers (Best Model)', fontsize=13)\n",
    "\n",
    "linear_layers = [(name, param) for name, param in best_model.named_parameters() \n",
    "                 if 'weight' in name and param.dim() == 2]\n",
    "\n",
    "# Sample up to 4 layers\n",
    "sample_layers = linear_layers[::max(1, len(linear_layers)//4)][:4]\n",
    "\n",
    "for ax, (name, param) in zip(axes, sample_layers):\n",
    "    weights = param.detach().cpu().numpy().flatten()\n",
    "    ax.hist(weights, bins=50, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "    ax.set_title(f\"{name}\\nμ={weights.mean():.4f}, σ={weights.std():.4f}\", fontsize=9)\n",
    "    ax.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/weight_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary of All Experiments\n",
    "\n",
    "Let's compile a comprehensive comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "all_experiments = {\n",
    "    **{f'Depth: {k}': v for k, v in depth_results.items()},\n",
    "    **{f'Width: {k}': v for k, v in width_results.items()},\n",
    "    **{f'Act: {k}': v for k, v in activation_results.items()},\n",
    "    **{f'Skip: {k}': v for k, v in skip_results.items()},\n",
    "    **{f'Opt: {k}': v for k, v in opt_results.items()},\n",
    "    **{f'Reg: {k}': v for k, v in reg_results.items()},\n",
    "    'Best Model': best_history\n",
    "}\n",
    "\n",
    "print(f\"{'Experiment':<40s} {'Best Val Acc':>12s} {'Best Epoch':>12s} {'Time (s)':>10s}\")\n",
    "print('=' * 76)\n",
    "for name, hist in sorted(all_experiments.items(), key=lambda x: -x[1]['best_val_acc']):\n",
    "    print(f\"{name:<40s} {hist['best_val_acc']:>11.2f}% {hist['best_epoch']:>10d} {hist['elapsed_time']:>10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Conclusions & Reflections\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Architecture:**\n",
    "- [Fill in after running: What depth worked best? Where did plain MLPs degrade?]\n",
    "- [How much did width matter vs. depth?]\n",
    "- [Did skip connections rescue deep models?]\n",
    "\n",
    "**Optimization:**\n",
    "- [Which optimizer performed best? Was it the fastest too?]\n",
    "- [Did learning rate schedules help?]\n",
    "- [Which regularization strategy was most effective?]\n",
    "\n",
    "**Interpretability:**\n",
    "- [Which class pairs were most confused? Why does this make intuitive sense?]\n",
    "- [How did the t-SNE representations differ between baseline and best model?]\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- We only explored MLPs — convolutional architectures (CNNs) would better exploit the spatial structure in images and likely outperform even our best MLP significantly.\n",
    "- Fashion-MNIST, while harder than MNIST, is still a relatively simple benchmark. Real-world image classification involves color, variable sizes, backgrounds, and occlusion.\n",
    "- We used a fixed train/test split rather than cross-validation, so our results have some variance.\n",
    "- Hyperparameter search was manual; Bayesian optimization or grid search might find better configurations.\n",
    "\n",
    "### Connection to Course Material\n",
    "\n",
    "This exploration directly connects to several concepts from the course:\n",
    "- **Gradient flow** (Lecture X): We saw empirically that deeper plain networks train worse, and skip connections solve this.\n",
    "- **Regularization** (Lecture X): Dropout, weight decay, and BatchNorm each address overfitting from a different angle.\n",
    "- **Representation learning** (Lecture X): The t-SNE visualizations show how networks transform raw pixels into linearly separable features.\n",
    "- **Bias-variance tradeoff**: Width and depth experiments directly demonstrate the tension between model capacity and generalization.\n",
    "\n",
    "### What I Would Do Next\n",
    "\n",
    "1. Implement a simple CNN and compare it to the MLP — the spatial structure in images should give a significant boost.\n",
    "2. Try data augmentation (random flips, rotations) to see how it affects generalization.\n",
    "3. Implement gradient-based interpretability (saliency maps) to see which pixels drive predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
